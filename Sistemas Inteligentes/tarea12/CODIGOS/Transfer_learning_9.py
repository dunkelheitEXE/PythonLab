{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # Changelog\n# \n# ## V38 - Update to mixed precision float16 training, TF 2.4\n# https://www.kaggle.com/devang/transfer-learning-with-keras-and-efficientnets\n# \n# ## V37 - Updates, switch to EfficientNets, TF 2.3\n# https://www.kaggle.com/devang/transfer-learning-with-keras-and-efficientnets?scriptVersionId=40698248\n# \n# ## V36 - Updates, works with TF 2.0\n# https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2?scriptVersionId=24113974\n# \n# ## V32 - Updates, added Confusion Matrix/Classification Report\n# https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2?scriptVersionId=19440127\n# \n# ## V24 - First working notebook\n# https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2?scriptVersionId=16125907\n# \n\n# %% [markdown]\n# # Imports\n\n# %% [code]\nimport os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n#os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"10\"\n#os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\nfrom platform import python_version\nimport warnings\nimport time\nimport datetime as dt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport multiprocessing as mp\nimport shutil\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\n# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input, decode_predictions\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sn\n\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nimport psutil\nimport random\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_policy(policy)\n\nprint(\"py\", python_version())\nprint(\"tf\", tf.__version__)\nprint(\"keras\", tf.keras.__version__)\nmem = psutil.virtual_memory()\nprint(\"mem\", mem.total/1024/1024)\ncpu = mp.cpu_count()\nprint(\"cpu\", cpu)\nprint('Compute dtype: %s' % policy.compute_dtype)\nprint('Variable dtype: %s' % policy.variable_dtype)\n\n%system nvidia-smi\n#%system rocm-smi\n\n# %% [markdown]\n# # Variables\n\n# %% [code]\nepochs = 100\nbatch_size = 100\ntestsplit = .2\ntargetx = 224\ntargety = 224\nlearning_rate = 0.0001\nclasses = 120\nseed = random.randint(1, 1000)\n\ndata_dir = \"/kaggle/input/images/Images/\"\nannotations_dir = \"/kaggle/input/annotations/Annotation/\"\ncropped_dir = \"/kaggle/working/cropped/\"\n\n# %% [markdown]\n# # Crop images using provided annotations\n\n# %% [code]\n%system rm -rf $cropped_dir\n%system mkdir $cropped_dir\n\n#this function adapted from https://www.kaggle.com/hengzheng/dog-breeds-classifier\ndef save_cropped_img(path, annotation, newpath):\n    tree = ET.parse(annotation)\n    xmin = int(tree.getroot().findall('.//xmin')[0].text)\n    xmax = int(tree.getroot().findall('.//xmax')[0].text)\n    ymin = int(tree.getroot().findall('.//ymin')[0].text)\n    ymax = int(tree.getroot().findall('.//ymax')[0].text)\n    image = Image.open(path)\n    image = image.crop((xmin, ymin, xmax, ymax))\n    image = image.convert('RGB')\n    image.save(newpath)\n\ndef crop_images():\n    breeds = os.listdir(data_dir)\n    annotations = os.listdir(annotations_dir)\n\n    print('breeds: ', len(breeds), 'annotations: ', len(annotations))\n\n    total_images = 0\n\n    for breed in breeds:\n        dir_list = os.listdir(data_dir + breed)\n        annotations_dir_list = os.listdir(annotations_dir + breed)\n        img_list = [data_dir + breed + '/' + i for i in dir_list]\n        os.makedirs(cropped_dir + breed)\n\n        for file in img_list:\n            annotation_path = annotations_dir + breed + '/' + os.path.basename(file[:-4])\n            newpath = cropped_dir + breed + '/' + os.path.basename(file)\n            save_cropped_img(file, annotation_path, newpath)\n            total_images += 1\n    \n    print(\"total images cropped\", total_images)\n\ncrop_images()\n\n# %% [markdown]\n# # Keras image data readers\n\n# %% [code]\ndatagen = ImageDataGenerator(\n        shear_range=0.1,\n        zoom_range=0.1,\n        brightness_range=[0.9,1.1],\n        horizontal_flip=True,\n        validation_split=testsplit,\n        preprocessing_function=preprocess_input\n)\n\ntrain_generator = datagen.flow_from_directory(\n        cropped_dir,\n        target_size=(targetx, targety),\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=True,\n        seed=seed,\n        subset=\"training\"\n)\n\ntest_generator = datagen.flow_from_directory(\n        cropped_dir,\n        target_size=(targetx, targety),\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=False,\n        seed=seed,\n        subset=\"validation\"\n)\n\n# %% [markdown]\n# # Sample image\n\n# %% [code]\nimg = train_generator.filepaths[np.random.random_integers(low=0, high=train_generator.samples)]\nprint(img)\nimg = mpimg.imread(img)\nplt.imshow(img)\n\n# %% [markdown]\n# # Keras callbacks\n\n# %% [code]\ncheckpoint = ModelCheckpoint('dog_breed_classifier.h5',\n                             monitor='val_accuracy',\n                             save_best_only=True,\n                             verbose=1,\n                             mode='auto',\n                             save_weights_only=False,\n                             period=1)\n\n#https://github.com/keras-team/keras/issues/3358\ntensorboard = TensorBoard(log_dir=\"./logs-\"+dt.datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n                            histogram_freq=0,\n                            batch_size=batch_size,\n                            write_graph=False,\n                            update_freq='epoch')\n\ndef epoch_end(epoch, logs):\n    message = \"End of epoch \"+str(epoch)+\". Learning rate: \"+str(K.eval(model.optimizer.lr))\n    os.system('echo '+message)\n\ndef epoch_begin(epoch, logs):\n    print(\"Learning rate: \", K.eval(model.optimizer.lr))\n    \ndef train_begin(logs):\n    os.system(\"echo Beginning training\")\n\nearlystop = EarlyStopping(monitor='val_accuracy',\n                          min_delta=.0001,\n                          patience=20,\n                          verbose=1,\n                          mode='auto',\n                          baseline=None,\n                          restore_best_weights=True)\n\nreducelr = ReduceLROnPlateau(monitor='val_accuracy',\n                             factor=np.sqrt(.1),\n                             patience=5,\n                             verbose=1,\n                             mode='auto',\n                             min_delta=.0001,\n                             cooldown=0,\n                             min_lr=0.0000001)\n\nlambdacb = LambdaCallback(on_epoch_begin=epoch_begin,\n                          on_epoch_end=epoch_end,\n                          on_batch_begin=None,\n                          on_batch_end=None,\n                          on_train_begin=train_begin,\n                          on_train_end=None)\n\n# %% [markdown]\n# # Define new top layers and compile model\n\n# %% [code]\n# base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\nbase_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# x = Dropout(rate = .2)(x)\nx = BatchNormalization()(x)\nx = Dense(1280, activation='relu',  kernel_initializer=glorot_uniform(seed), bias_initializer='zeros')(x)\n# x = Dropout(rate = .2)(x)\nx = BatchNormalization()(x)\npredictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\noptimizer = Adam(lr=learning_rate)\n# optimizer = RMSprop(lr=learning_rate)\n\nloss = \"categorical_crossentropy\"\n# loss = \"kullback_leibler_divergence\"\n\nfor layer in model.layers:\n    layer.trainable = True\n# for layer in model.layers[-2:]:\n#     layer.trainable = True\n\nmodel.compile(optimizer=optimizer,\n              loss=loss,\n              metrics=[\"accuracy\"])\n\nmodel.summary()\nfor i, layer in enumerate(model.layers):\n    print(i, layer.name, layer.trainable)\n\n# %% [markdown]\n# # Fit model\n\n# %% [code]\n%%time\n\nparams = model.fit_generator(generator=train_generator, \n                                steps_per_epoch=len(train_generator), \n                                validation_data=test_generator, \n                                validation_steps=len(test_generator),\n                                epochs=epochs,\n                                callbacks=[reducelr, earlystop, lambdacb, tensorboard, checkpoint])\n\n# %% [markdown]\n# # Training and test loss/accuracy graphs\n\n# %% [code]\nplt.subplot(1, 2, 1)\nplt.title('Training and test accuracy')\nplt.plot(params.epoch, params.history['accuracy'], label='Training accuracy')\nplt.plot(params.epoch, params.history['val_accuracy'], label='Test accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.title('Training and test loss')\nplt.plot(params.epoch, params.history['loss'], label='Training loss')\nplt.plot(params.epoch, params.history['val_loss'], label='Test loss')\nplt.legend()\n\nplt.show()\n\n# %% [markdown]\n# # Sample prediction\n\n# %% [code]\n# Randomly test an image from the test set\n# model.load_weights('dog_breed_classifier.h5')\n\nimageno=np.random.random_integers(low=0, high=test_generator.samples)\n\nname = test_generator.filepaths[imageno]\nprint(name)\nplt.imshow(mpimg.imread(name))\n\nimg = Image.open(test_generator.filepaths[imageno]).resize((targetx, targety))\nprobabilities = model.predict(preprocess_input(np.expand_dims(img, axis=0)))\nbreed_list = tuple(zip(test_generator.class_indices.values(), test_generator.class_indices.keys()))\n\nfor i in probabilities[0].argsort()[-5:][::-1]: \n    print(probabilities[0][i], \"  :  \" , breed_list[i])\n\n# %% [markdown]\n# # Classification report\n\n# %% [code]\ntest_generator.reset()\npredictions = model.predict_generator(test_generator, steps=len(test_generator))\ny = np.argmax(predictions, axis=1)\n\nprint('Classification Report')\ncr = classification_report(y_true=test_generator.classes, y_pred=y, target_names=test_generator.class_indices)\nprint(cr)\n\n# %% [markdown]\n# # Confusion matrix\n\n# %% [code]\nprint('Confusion Matrix')\ncm = confusion_matrix(test_generator.classes, y)\ndf = pd.DataFrame(cm, columns=test_generator.class_indices)\nplt.figure(figsize=(80,80))\nsn.heatmap(df, annot=True)\n\n# %% [code]\nshutil.rmtree(cropped_dir)","metadata":{"_uuid":"8bfa78c2-30da-4a60-844a-85cfa6472ddf","_cell_guid":"7058e586-c7de-4233-b542-692b80ca4f40","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-27T02:16:29.982789Z","iopub.execute_input":"2021-06-27T02:16:29.983187Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"py 3.7.9\ntf 2.4.1\nkeras 2.4.0\nmem 18009.1875\ncpu 4\nCompute dtype: float16\nVariable dtype: float32\nbreeds:  120 annotations:  120\n","output_type":"stream"}]}]}
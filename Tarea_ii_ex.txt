# %% [1] Importar librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, RocCurveDisplay
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor

# %% [2] Cargar y describir el dataset
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

print("Descripción del dataset:")
print(f"Nombre: Iris")
print(f"Muestras: {df.shape[0]}")
print(f"Variables: {df.shape[1]-1}")
print("Variables independientes:")
print(iris.feature_names)
print(f"Tipo de variables: Todas numéricas")
print("Clases:", iris.target_names)
print("Distribución de clases:")
print(df['target'].value_counts())

# %% [3] Preprocesamiento
# Dividir datos
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Estandarización
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Análisis de normalidad
print("\nPruebas de normalidad (Shapiro-Wilk):")
for i, col in enumerate(X.columns):
    stat, p = stats.shapiro(X_train_scaled[:, i])
    print(f"{col}: p-value = {p:.4f}")

# Análisis de multicolinealidad
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif["Variable"] = X.columns
print("\nFactores de inflación de varianza (VIF):")
print(vif)

# %% [4] Aplicación de PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_train_pca[:,0], y=X_train_pca[:,1], hue=y_train, palette='viridis')
plt.title('Proyección PCA de los datos')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# %% [5] Aplicación de LDA
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_scaled, y_train)
X_test_lda = lda.transform(X_test_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_train_lda[:,0], y=X_train_lda[:,1], hue=y_train, palette='viridis')
plt.title('Proyección LDA de los datos')
plt.xlabel('Componente Discriminante 1')
plt.ylabel('Componente Discriminante 2')
plt.show()

# %% [6] Modelos de regresión logística
# PCA
lr_pca = LogisticRegression(max_iter=200)
lr_pca.fit(X_train_pca, y_train)
y_pred_pca = lr_pca.predict(X_test_pca)

# LDA
lr_lda = LogisticRegression(max_iter=200)
lr_lda.fit(X_train_lda, y_train)
y_pred_lda = lr_lda.predict(X_test_lda)

# %% [7] Evaluación de modelos
# Precisión
print(f"\nPrecisión PCA: {accuracy_score(y_test, y_pred_pca):.4f}")
print(f"Precisión LDA: {accuracy_score(y_test, y_pred_lda):.4f}")

# Matrices de confusión
fig, ax = plt.subplots(1, 2, figsize=(15, 6))
conf_matrix_pca = confusion_matrix(y_test, y_pred_pca)
conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)

sns.heatmap(conf_matrix_pca, annot=True, fmt='d', ax=ax[0], cmap='Blues')
ax[0].set_title('Matriz de Confusión PCA')
sns.heatmap(conf_matrix_lda, annot=True, fmt='d', ax=ax[1], cmap='Blues')
ax[1].set_title('Matriz de Confusión LDA')
plt.show()

# Curvas ROC
fig, ax = plt.subplots(1, 2, figsize=(15, 6))
RocCurveDisplay.from_estimator(lr_pca, X_test_pca, y_test, ax=ax[0])
ax[0].set_title('Curva ROC PCA')
RocCurveDisplay.from_estimator(lr_lda, X_test_lda, y_test, ax=ax[1])
ax[1].set_title('Curva ROC LDA')
plt.show()

# Curvas de aprendizaje
def plot_learning_curve(estimator, title, X, y, ax):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10))
    
    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label="Entrenamiento")
    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label="Validación")
    ax.set_title(title)
    ax.set_xlabel("Muestras de entrenamiento")
    ax.set_ylabel("Precisión")
    ax.legend()

fig, ax = plt.subplots(1, 2, figsize=(15, 6))
plot_learning_curve(lr_pca, "Curva de aprendizaje PCA", X_train_pca, y_train, ax[0])
plot_learning_curve(lr_lda, "Curva de aprendizaje LDA", X_train_lda, y_train, ax[1])
plt.show()

# %% [8] Visualización adicional
# Varianza explicada PCA
plt.figure(figsize=(8, 5))
plt.bar(range(2), pca.explained_variance_ratio_)
plt.title('Varianza explicada por componente (PCA)')
plt.xlabel('Componentes principales')
plt.ylabel('Varianza explicada')
plt.xticks([0, 1], ['PC1', 'PC2'])
plt.show()

# Comparación de precisión
plt.figure(figsize=(8, 5))
plt.bar(['PCA', 'LDA'], [accuracy_score(y_test, y_pred_pca), accuracy_score(y_test, y_pred_lda)])
plt.title('Comparación de precisión entre modelos')
plt.ylabel('Precisión')
plt.ylim(0.8, 1.0)
plt.show()